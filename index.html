---
layout: home
title: Sci-Assess
subtitle: SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis
---

<!-- <img src="assets/img/Uni-SMART-framework.png" alt="Descriptive Alt Text" style="width:100%;"> -->

<body>
    <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Recent breakthroughs in Large Language Models (LLMs) have revolutionized natural language understanding and generation, igniting a surge of interest in leveraging these technologies for the nuanced field of scientific literature analysis. 
              Existing benchmarks, however, inadequately evaluate the proficiency of LLMs in the scientific domain, especially in scenarios involving complex comprehension and multimodal data.
              In response, we introduced SciAssess, a benchmark tailored for the in-depth analysis of scientific literature, crafted to provide a thorough assessment of LLMs' efficacy.
              SciAssess focuses on evaluating LLMs' abilities in memorization, comprehension, and analysis within scientific contexts. 
              It includes representative tasks from diverse scientific fields, such as general chemistry, organic materials, and alloy materials. 
              And rigorous quality control measures ensure its reliability in terms of correctness, anonymization, and copyright compliance. 
              SciAssess evaluates leading LLMs, including GPT-4, GPT-3.5-turbo, and Gemini, identifying their strengths and areas for improvement and supporting the ongoing development of LLM applications in scientific literature analysis. 
              SciAssess and its resources are made available at <a href="https://sci-assess.github.io/">https://sci-assess.github.io/</a>, offering a valuable tool for advancing LLM capabilities in scientific literature analysis.
            </p>
          </div>
          <h2 class="title is-3">Code</h2>
          <div class="content has-text-justified">
            <p>
              Our code, essential for replicating the studies and experiments outlined in SciAssess, is currently in the final stages of preparation. It will soon be made publicly available, allowing researchers and developers alike to contribute to and benefit from the advancements in LLMs for scientific literature analysis. Stay tuned for updates.
            </p>
          </div>
        </div>
      </div>
    </div>
    </section>
</body>
